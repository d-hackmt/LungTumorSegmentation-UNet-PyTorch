{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "corresponding-replication",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Finally we are going to train our tumor segmentation network. <br />\n",
    "Here we apply some small changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-bahamas",
   "metadata": {},
   "source": [
    "## Imports:\n",
    "\n",
    "* Pathlib for easy path handling\n",
    "* torch for tensor handling\n",
    "* pytorch lightning for efficient and easy training implementation\n",
    "* ModelCheckpoint and TensorboardLogger for checkpoint saving and logging\n",
    "* imgaug for Data Augmentation\n",
    "* numpy for file loading and array ops\n",
    "* matplotlib for visualizing some images\n",
    "* tqdm for progress par when validating the model\n",
    "* celluloid for easy video generation\n",
    "* Our dataset and model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pregnant-density",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import imgaug.augmenters as iaa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from celluloid import Camera\n",
    "\n",
    "from dataset import LungDataset\n",
    "from model import UNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-implementation",
   "metadata": {},
   "source": [
    "## Dataset Creation\n",
    "Here we create the train and validation dataset. <br />\n",
    "Additionally we define our data augmentation pipeline.\n",
    "Subsequently the two dataloaders are created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "broadband-arcade",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = iaa.Sequential([\n",
    "    iaa.Affine(translate_percent=(0.15), \n",
    "               scale=(0.85, 1.15), # zoom in or out\n",
    "               rotate=(-45, 45)#\n",
    "               ),  # rotate up to 45 degrees\n",
    "    iaa.ElasticTransformation()  # Elastic Transformations\n",
    "                ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "laughing-rehabilitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8978 train images and 0 val images\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset objects\n",
    "train_path = Path(\"Task06_Lung/Preprocessed/train/\")\n",
    "val_path = Path(\"Task06_Lung/Preprocessed/val/\")\n",
    "\n",
    "train_dataset = LungDataset(train_path, seq)\n",
    "val_dataset = LungDataset(val_path, None)\n",
    "\n",
    "print(f\"There are {len(train_dataset)} train images and {len(val_dataset)} val images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-theater",
   "metadata": {},
   "source": [
    "## Oversampling to tackle strong class imbalance\n",
    "Lung tumors are often very small, thus we need to make sure that our model does not learn a trivial solution which simply outputs 0 for all voxels.<br />\n",
    "In this notebook we will use oversampling to sample slices which contain a tumor more often.\n",
    "\n",
    "To do so we can use the **WeightedRandomSampler** provided by pytorch which needs a weight for each sample in the dataset.\n",
    "Typically you have one weight for each class, which means that we need to calculate two weights, one for slices without tumors and one for slices with a tumor and create list that assigns each sample from the dataset the corresponding weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-nashville",
   "metadata": {},
   "source": [
    "To do so, we at first need to create a list containing only the class labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "comic-fence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "155d4b18f9a54dafaa714fa6df32d11d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8978 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaikr\\.conda\\envs\\Jupyter_1\\lib\\site-packages\\imgaug\\imgaug.py:106: DeprecationWarning: Got a float array as the segmentation map in SegmentationMapsOnImage. That is deprecated. Please provide instead a (H,W,[C]) array of dtype bool_, int or uint, where C denotes the segmentation map index.\n",
      "  warn(msg, category=DeprecationWarning, stacklevel=stacklevel)\n"
     ]
    }
   ],
   "source": [
    "target_list = []\n",
    "for _, label in tqdm(train_dataset):\n",
    "    # Check if mask contains a tumorous pixel:\n",
    "    if np.any(label):\n",
    "        target_list.append(1)\n",
    "    else:\n",
    "        target_list.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-antenna",
   "metadata": {},
   "source": [
    "Then we can calculate the weight for each class:\n",
    "To do so, we can simply compute the fraction between the classes and then create the weight list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "thirty-connectivity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([8037,  941], dtype=int64))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniques = np.unique(target_list, return_counts=True)\n",
    "uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bridal-advantage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.540913921360255"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraction = uniques[1][0] / uniques[1][1]\n",
    "fraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-brunswick",
   "metadata": {},
   "source": [
    "Subsequently we assign the weight 1 to each slice without a tumor and ~9 to each slice with a tumor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "planned-stanford",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_list = []\n",
    "for target in target_list:\n",
    "    if target == 0:\n",
    "        weight_list.append(1)\n",
    "    else:\n",
    "        weight_list.append(fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "charitable-detroit",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_list[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-tampa",
   "metadata": {},
   "source": [
    "Finally we create the sampler which we can pass to the DataLoader.\n",
    "**Important:** Only use a sampler for the train loader! We don't want to change the validation data to get a real validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "backed-acoustic",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = torch.utils.data.sampler.WeightedRandomSampler(weight_list, len(weight_list))                     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "final-assets",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8#TODO\n",
    "num_workers = 4# TODO\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                           num_workers=num_workers, sampler=sampler)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-growth",
   "metadata": {},
   "source": [
    "We can verify that our sampler works by taking a batch from the train loader and count how many labels are larger than zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "unauthorized-wesley",
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_sampler = next(iter(train_loader))  # Take one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bright-glasgow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False,  True,  True,  True, False, False,  True,  True])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(verify_sampler[1][:,0]).sum([1, 2]) > 0  # ~ half the batch size "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-startup",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "As this is a harder task to train you might try different loss functions:\n",
    "We achieved best results by using the Binary Cross Entropy instead of the Dice Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-radio",
   "metadata": {},
   "source": [
    "## Full Segmentation Model\n",
    "\n",
    "We now combine everything into the full pytorch lightning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "little-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TumorSegmentation(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = UNet()\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)\n",
    "        self.loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def forward(self, data):\n",
    "        pred = self.model(data)\n",
    "        return pred\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        ct, mask = batch\n",
    "        mask = mask.float()\n",
    "        \n",
    "        pred = self(ct.float())\n",
    "        loss = self.loss_fn(pred, mask)\n",
    "        \n",
    "        # Logs\n",
    "        self.log(\"Train Dice\", loss)\n",
    "        if batch_idx % 50 == 0:\n",
    "            self.log_images(ct.cpu(), pred.cpu(), mask.cpu(), \"Train\")\n",
    "        return loss\n",
    "    \n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        ct, mask = batch\n",
    "        mask = mask.float()\n",
    "\n",
    "        pred = self(ct.float())\n",
    "        loss = self.loss_fn(pred, mask)\n",
    "        \n",
    "        # Logs\n",
    "        self.log(\"Val Dice\", loss)\n",
    "        if batch_idx % 50 == 0:\n",
    "            self.log_images(ct.cpu(), pred.cpu(), mask.cpu(), \"Val\")\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def log_images(self, ct, pred, mask, name):\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        pred = pred > 0.5 # As we use the sigomid activation function, we threshold at 0.5\n",
    "        \n",
    "        \n",
    "        fig, axis = plt.subplots(1, 2)\n",
    "        axis[0].imshow(ct[0][0], cmap=\"bone\")\n",
    "        mask_ = np.ma.masked_where(mask[0][0]==0, mask[0][0])\n",
    "        axis[0].imshow(mask_, alpha=0.6)\n",
    "        axis[0].set_title(\"Ground Truth\")\n",
    "        \n",
    "        axis[1].imshow(ct[0][0], cmap=\"bone\")\n",
    "        mask_ = np.ma.masked_where(pred[0][0]==0, pred[0][0])\n",
    "        axis[1].imshow(mask_, alpha=0.6, cmap=\"autumn\")\n",
    "        axis[1].set_title(\"Pred\")\n",
    "\n",
    "        self.logger.experiment.add_figure(f\"{name} Prediction vs Label\", fig, self.global_step)\n",
    "\n",
    "            \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        #Caution! You always need to return a list here (just pack your optimizer into one :))\n",
    "        return [self.optimizer]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "featured-nature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate the model\n",
    "model = TumorSegmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "diagnostic-particle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='Val Dice',\n",
    "    save_top_k=30,\n",
    "    mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "pacific-inventory",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Create the trainer\n",
    "# Change the gpus parameter to the number of available gpus in your computer. Use 0 for CPU training\n",
    "\n",
    "gpus = 1 #TODO\n",
    "trainer = pl.Trainer(logger=TensorBoardLogger(save_dir=\"./logs\"), log_every_n_steps=1,\n",
    "                     callbacks=checkpoint_callback,\n",
    "                     max_epochs=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "handy-testing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name    | Type              | Params\n",
      "----------------------------------------------\n",
      "0 | model   | UNet              | 7.8 M \n",
      "1 | loss_fn | BCEWithLogitsLoss | 0     \n",
      "----------------------------------------------\n",
      "7.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.8 M     Total params\n",
      "31.127    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                               | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51db23633e04c4d808abfb88685ec38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                      | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaikr\\.conda\\envs\\Jupyter_1\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-harvest",
   "metadata": {},
   "source": [
    "## Evaluation:\n",
    "Let's evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "challenging-color",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceScore(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    class to compute the Dice Loss\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, pred, mask):\n",
    "                \n",
    "        #flatten label and prediction tensors\n",
    "        pred = torch.flatten(pred)\n",
    "        mask = torch.flatten(mask)\n",
    "        \n",
    "        counter = (pred * mask).sum()  # Counter       \n",
    "        denum = pred.sum() + mask.sum()  # denominator\n",
    "        dice = (2*counter)/denum\n",
    "        \n",
    "        return dice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "19d25bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Udemy-Medical PyTorch\\\\07-CAPSTONE-PROJECT--Lung-Tumor-Segmentation\\\\07-CAPSTONE-PROJECT--Lung-Tumor-Segmentation\\\\Solution'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "initial-strain",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaikr\\.conda\\envs\\Jupyter_1\\lib\\site-packages\\pytorch_lightning\\utilities\\migration\\migration.py:207: You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys that would end up colliding with each other after an upgrade, which means we can't differentiate which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint` callbacks will not be able to reload the state.\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.3.4 to v2.1.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint D:\\Udemy-Medical PyTorch\\07-CAPSTONE-PROJECT--Lung-Tumor-Segmentation\\07-CAPSTONE-PROJECT--Lung-Tumor-Segmentation\\checkpoints\\epoch=29-step=53759.ckpt`\n"
     ]
    }
   ],
   "source": [
    "model = TumorSegmentation.load_from_checkpoint(\"../checkpoints/epoch=29-step=53759.ckpt\")\n",
    "model.eval();\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "imported-neighborhood",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2d194d94d724e5cb485a528a390b897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = []\n",
    "labels = []\n",
    "\n",
    "for slice, label in tqdm(val_dataset):\n",
    "    slice = torch.tensor(slice).float().to(device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        pred = torch.sigmoid(model(slice))\n",
    "    preds.append(pred.cpu().numpy())\n",
    "    labels.append(label)\n",
    "    \n",
    "preds = np.array(preds)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-fourth",
   "metadata": {},
   "source": [
    "Compute overall Dice Score: This is not a bad result!\n",
    "Those tumors are extremely small and already some wrongly segmented pixels strongly reduce the Dice Score.\n",
    "The Visualization below demonstrates that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "breathing-oregon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Val Dice Score is: nan\n"
     ]
    }
   ],
   "source": [
    "dice_score = DiceScore()(torch.from_numpy(preds), torch.from_numpy(labels).unsqueeze(0).float())\n",
    "print(f\"The Val Dice Score is: {dice_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-extent",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Feel free to play around with the threshold.\n",
    "\n",
    "What happens if you decrease it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "central-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "wound-terrorism",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "prime-regard",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = Path(\"Task06_Lung/imagesTs/lung_013.nii.gz\")\n",
    "ct = nib.load(subject).get_fdata() / 3071  # standardize\n",
    "ct = ct[:,:,30:]  # crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "unnecessary-accreditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation = []\n",
    "label = []\n",
    "scan = []\n",
    "\n",
    "for i in range(ct.shape[-1]):\n",
    "    slice = ct[:,:,i]\n",
    "    slice = cv2.resize(slice, (256, 256))\n",
    "    slice = torch.tensor(slice)\n",
    "    scan.append(slice)\n",
    "    slice = slice.unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = model(slice)[0][0].cpu()\n",
    "    pred = pred > THRESHOLD\n",
    "    segmentation.append(pred)\n",
    "    label.append(segmentation)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-elimination",
   "metadata": {},
   "source": [
    "Plotting the predicted segmentation (red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "desperate-scholar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "camera = Camera(fig)  # create the camera object from celluloid\n",
    "\n",
    "for i in range(0, len(scan), 2):  # Sagital view. Skip every second slice to reduce the video length\n",
    "    plt.imshow(scan[i], cmap=\"bone\")\n",
    "    mask = np.ma.masked_where(segmentation[i]==0, segmentation[i])\n",
    "    plt.imshow(mask, alpha=0.5, cmap=\"autumn\")  # Use autumn colormap to get red segmentation \n",
    "    \n",
    "#     plt.axis(\"off\")\n",
    "    camera.snap()  # Store the current slice\n",
    "animation = camera.animate()  # create the animation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "periodic-occasion",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Requested MovieWriter (ffmpeg) not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTML\n\u001b[1;32m----> 2\u001b[0m HTML(\u001b[43manimation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_html5_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# convert the animation to a video\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\Jupyter_1\\lib\\site-packages\\matplotlib\\animation.py:1285\u001b[0m, in \u001b[0;36mAnimation.to_html5_video\u001b[1;34m(self, embed_limit)\u001b[0m\n\u001b[0;32m   1282\u001b[0m path \u001b[38;5;241m=\u001b[39m Path(tmpdir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemp.m4v\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;66;03m# We create a writer manually so that we can get the\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;66;03m# appropriate size for the tag\u001b[39;00m\n\u001b[1;32m-> 1285\u001b[0m Writer \u001b[38;5;241m=\u001b[39m \u001b[43mwriters\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrcParams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43manimation.writer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1286\u001b[0m writer \u001b[38;5;241m=\u001b[39m Writer(codec\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh264\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1287\u001b[0m                 bitrate\u001b[38;5;241m=\u001b[39mmpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manimation.bitrate\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   1288\u001b[0m                 fps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000.\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interval)\n\u001b[0;32m   1289\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mstr\u001b[39m(path), writer\u001b[38;5;241m=\u001b[39mwriter)\n",
      "File \u001b[1;32m~\\.conda\\envs\\Jupyter_1\\lib\\site-packages\\matplotlib\\animation.py:148\u001b[0m, in \u001b[0;36mMovieWriterRegistry.__getitem__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_available(name):\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registered[name]\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested MovieWriter (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) not available\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Requested MovieWriter (ffmpeg) not available"
     ]
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(animation.to_html5_video())  # convert the animation to a video\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-function",
   "metadata": {},
   "source": [
    "Congratulations! You created a lung cancer segmentation model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
